{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#乱七八糟版第四题\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.integrate import solve_ivp, cumulative_trapezoid as cumtrapz\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. Soft_LQR 类（Exercise 2 实现的代码）——改进版\n",
    "# ==========================================\n",
    "class Soft_LQR:\n",
    "    def __init__(self, H, M, C, D, R, sigma, T, N, tau, gamma):\n",
    "        \"\"\"\n",
    "        初始化 soft LQR 类\n",
    "        参数同前，计算 S(t) 并提供最优值函数 v*(t,x)\n",
    "        优化：预先计算积分项 I(t)=∫[t,T] tr(σσ^T S(r)) dr\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.R = R\n",
    "        self.sigma = sigma\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.time_grid = torch.linspace(0, T, N + 1)\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.S_values = self.solve_riccati_ode()\n",
    "        self.integral_values = self.precompute_integrals()\n",
    "\n",
    "    def riccati_ode(self, t, S_flat):\n",
    "        S = torch.tensor(S_flat, dtype=torch.float32).reshape(2, 2)\n",
    "        D_term = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        S_dot = S.T @ self.M @ torch.linalg.inv(D_term) @ self.M.T @ S - self.H.T @ S - S @ self.H - self.C\n",
    "        return S_dot.flatten()\n",
    "\n",
    "    def solve_riccati_ode(self):\n",
    "        S_T = self.R.flatten()\n",
    "        indices = torch.arange(self.time_grid.size(0) - 1, -1, -1)\n",
    "        time_grid_re = torch.index_select(self.time_grid, 0, indices)\n",
    "        sol = solve_ivp(self.riccati_ode, [self.T, 0], S_T, t_eval=time_grid_re, atol=1e-10, rtol=1e-10)\n",
    "        S_matrices = sol.y.T[::-1].reshape(-1, 2, 2)\n",
    "        # 构造字典：key 为时间（浮点数），value 为 S 矩阵（numpy 数组）\n",
    "        return dict(zip(tuple(self.time_grid.tolist()), S_matrices))\n",
    "\n",
    "    def precompute_integrals(self):\n",
    "        \"\"\"\n",
    "        对于每个时间点 t_i 在 time_grid 上，预先计算\n",
    "         I(t_i) = ∫_{t_i}^{T} tr(σσ^T S(r)) dr\n",
    "        使用 trapezoidal 规则进行数值积分。\n",
    "        返回字典：key 为 t_i，value 为 I(t_i)（标量 float）\n",
    "        \"\"\"\n",
    "        times = self.time_grid.numpy()  # shape (N+1,)\n",
    "        # 计算每个时间点对应的 f(t)= tr(σσ^T S(t))\n",
    "        f_vals = []\n",
    "        for t in times:\n",
    "            S_t = self.S_values[t]\n",
    "            f_val = torch.trace(self.sigma @ self.sigma.T @ torch.tensor(S_t, dtype=torch.float32)).item()\n",
    "            f_vals.append(f_val)\n",
    "        f_vals = np.array(f_vals)\n",
    "        dt = times[1] - times[0]\n",
    "        # 使用 cumulative_trapezoid 求积分，并传入 initial=0 使得输出长度与输入一致\n",
    "        cum_int = cumtrapz(f_vals[::-1], dx=dt, initial=0)[::-1]\n",
    "        integral_dict = {t: I for t, I in zip(times, cum_int)}\n",
    "        return integral_dict\n",
    "\n",
    "    def get_nearest_S(self, t):\n",
    "        nearest_t = self.time_grid[torch.argmin(torch.abs(self.time_grid - t))]\n",
    "        return self.S_values[nearest_t.tolist()]\n",
    "\n",
    "    def value_function(self, t, x):\n",
    "        \"\"\"\n",
    "        计算 v*(t,x)= x^T S(t)x + I(t) + (T-t)*C_{D,tau,gamma}\n",
    "        其中 I(t)=∫[t,T] tr(σσ^T S(r)) dr，\n",
    "        C_{D,tau,gamma} = -tau * ln((tau/gamma^2)*sqrt(det((D+tau/(2*gamma^2)I)^{-1})))\n",
    "        \"\"\"\n",
    "        S_t = self.get_nearest_S(t)\n",
    "        S_t = torch.tensor(S_t, dtype=torch.float32)\n",
    "        val = x.T @ S_t @ x\n",
    "\n",
    "        # 使用预先计算的积分值\n",
    "        times = self.time_grid.numpy()\n",
    "        # 找到最接近 t 的时间点\n",
    "        idx = np.argmin(np.abs(times - t))\n",
    "        t_nearest = times[idx]\n",
    "        val = val + self.integral_values[t_nearest]\n",
    "\n",
    "        var_matrix = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        inv_matrix = torch.linalg.inv(var_matrix)\n",
    "        det_matrix = torch.det(inv_matrix)\n",
    "        C_const = - self.tau * torch.log((self.tau / self.gamma ** 2) * torch.sqrt(det_matrix))\n",
    "        val = val + (self.T - t) * C_const\n",
    "        return val\n",
    "\n",
    "    def optimal_control(self, t, x):\n",
    "        \"\"\"\n",
    "        返回最优控制分布： N( -inv(D+tau/(2*gamma^2)I) * M^T S(t)x, tau*(D+tau/(2*gamma^2)I) )\n",
    "        \"\"\"\n",
    "        S_t = self.get_nearest_S(t)\n",
    "        S_t = torch.tensor(S_t, dtype=torch.float32)\n",
    "        inv_term = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        mean_control = -torch.linalg.inv(inv_term) @ self.M.T @ S_t @ x\n",
    "        cov_control = self.tau * inv_term\n",
    "        control_dist = MultivariateNormal(mean_control, cov_control)\n",
    "        return control_dist\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Actor 策略：参数化为线性映射 θ (2x2矩阵)\n",
    "# ==========================================\n",
    "class Actor:\n",
    "    def __init__(self, state_dim=2, action_dim=2):\n",
    "        # 参数 theta 初始化为随机 2x2 矩阵（可学习）\n",
    "        self.theta = torch.nn.Parameter(torch.randn(action_dim, state_dim, dtype=torch.float32))\n",
    "\n",
    "    def get_action(self, x):\n",
    "        \"\"\"\n",
    "        给定状态 x (2维列向量)，输出策略均值 μ = θ x，\n",
    "        并 squeeze 成一维向量\n",
    "        \"\"\"\n",
    "        return (self.theta @ x).squeeze()\n",
    "\n",
    "    def get_distribution(self, x, cov):\n",
    "        mu = self.get_action(x)\n",
    "        return MultivariateNormal(mu, cov)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. 利用当前 actor 策略生成一条轨迹（用 Euler 离散化）\n",
    "# ==========================================\n",
    "\n",
    "    \"\"\"\n",
    "    输入：\n",
    "      actor：当前策略（Actor 实例）\n",
    "      H, M, sigma：系统矩阵（torch 张量）\n",
    "      dt, T：时间步长和终止时间\n",
    "      x0：初始状态（torch 列向量 shape (2,1)）\n",
    "      cov：策略固定协方差（torch 张量，2x2）\n",
    "\n",
    "    返回：\n",
    "      traj: 状态列表 (每个元素为 2x1 tensor)\n",
    "      log_probs: 列表，每步动作的 log 概率（标量 tensor）\n",
    "      costs: 列表，每步即时代价 f = x^T C x + a^T D a （标量 tensor）\n",
    " \"\"\"\n",
    "def simulate_actor_episode(actor, H, M, sigma, dt, T, x0, cov):\n",
    "    traj = [x0.detach().clone()]  # x0 的形状现在为 (2,)\n",
    "    log_probs = []\n",
    "    costs = []\n",
    "\n",
    "    x = x0\n",
    "    num_steps = int(T / dt)\n",
    "    for _ in range(num_steps):\n",
    "        # 1) 生成动作，a 的形状为 (2,)\n",
    "        dist = actor.get_distribution(x, cov)\n",
    "        a = dist.rsample()  # 不再 unsqueeze\n",
    "        logp = dist.log_prob(a)\n",
    "        log_probs.append(logp)\n",
    "\n",
    "        # 2) 用 no_grad() 包裹环境更新和 cost 计算\n",
    "        dt_tensor = torch.tensor(dt, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            # 计算即时 cost，注意：x 和 a 均为 1D 向量，所以用 x @ A @ x\n",
    "            cost = (x @ C_tensor @ x) + (a @ D_tensor @ a)\n",
    "            costs.append(cost.item())\n",
    "            # 生成噪声也为一维\n",
    "            noise = torch.randn(2)\n",
    "            # 状态更新：x、a、noise 均为形状 (2,)\n",
    "            x = x + (H @ x + M @ a) * dt + sigma @ (noise * torch.sqrt(dt_tensor))\n",
    "\n",
    "        # 3) 保存新的状态\n",
    "        traj.append(x.detach().clone())\n",
    "\n",
    "    return traj, log_probs, costs\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Actor-only 算法主训练函数\n",
    "# ==========================================\n",
    "def train_actor(num_episodes=500, dt=0.01, T=1.0, gamma_lr=1e-8):  # 默认学习率改为 1e-3\n",
    "    # 固定环境参数（与 main() 中一致）\n",
    "    H_np = np.array([[0.5, 0.5],\n",
    "                     [0.0, 0.5]], dtype=np.float32)\n",
    "    M_np = np.array([[1.0, 1.0],\n",
    "                     [0.0, 1.0]], dtype=np.float32)\n",
    "    sigma_np = np.eye(2, dtype=np.float32) * 0.5\n",
    "    C_np = np.array([[1.0, 0.1],\n",
    "                     [0.1, 1.0]], dtype=np.float32)\n",
    "    D_np = np.array([[1.0, 0.1],\n",
    "                     [0.1, 1.0]], dtype=np.float32) * 0.1\n",
    "    R_terminal_np = np.array([[1.0, 0.3],\n",
    "                              [0.3, 1.0]], dtype=np.float32) * 10.0\n",
    "    tau = 0.1\n",
    "    gamma_param = 10.0\n",
    "    N_steps = int(T / dt)\n",
    "\n",
    "    # 转换为 torch 张量\n",
    "    H_tensor = torch.tensor(H_np)\n",
    "    M_tensor = torch.tensor(M_np)\n",
    "    sigma_tensor = torch.tensor(sigma_np)\n",
    "    global C_tensor, D_tensor\n",
    "    C_tensor = torch.tensor(C_np)\n",
    "    D_tensor = torch.tensor(D_np)\n",
    "\n",
    "    # 初始化 Soft_LQR\n",
    "    soft_lqr = Soft_LQR(H_tensor, M_tensor, C_tensor, D_tensor, torch.tensor(R_terminal_np), sigma_tensor, T, N_steps,\n",
    "                        tau, gamma_param)\n",
    "\n",
    "    # 固定策略协方差\n",
    "    var_matrix = D_tensor + tau / (2 * (gamma_param ** 2)) * torch.eye(2)\n",
    "    cov_opt = tau * var_matrix\n",
    "\n",
    "    # 初始化 Actor 和优化器\n",
    "    actor = Actor(state_dim=2, action_dim=2)\n",
    "    optimizer = torch.optim.AdamW([actor.theta], lr=gamma_lr, weight_decay=1e-4)  # 改用 AdamW\n",
    "\n",
    "    # 定义初始状态生成函数\n",
    "    def get_init_state():\n",
    "    # 返回形状为 (2,) 的向量\n",
    "        return torch.tensor([np.random.uniform(-2, 2), np.random.uniform(-2, 2)], dtype=torch.float32)\n",
    "\n",
    "\n",
    "    error_list = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        init_state = get_init_state()  # 随机初始状态\n",
    "        traj, log_probs, costs = simulate_actor_episode(actor, H_tensor, M_tensor, sigma_tensor, dt, T, init_state,\n",
    "                                                        cov_opt)\n",
    "\n",
    "        # 计算 v* 和 delta_v\n",
    "        v_star = [soft_lqr.value_function(i * dt, x) for i, x in enumerate(traj)]\n",
    "        delta_v = [v_star[i + 1] - v_star[i] for i in range(len(v_star) - 1)]\n",
    "\n",
    "        # 计算损失\n",
    "        loss = 0\n",
    "        for n in range(len(delta_v)):\n",
    "    \n",
    "\n",
    "         loss_term = log_probs[n] * delta_v[n] \\\n",
    "            + log_probs[n] * costs[n] * dt \\\n",
    "            + tau * (log_probs[n]**2) * dt\n",
    "         loss -= loss_term\n",
    "        loss = loss.sum()\n",
    "\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算误差\n",
    "        S0 = torch.tensor(soft_lqr.get_nearest_S(0), dtype=torch.float32)\n",
    "        mu_star = -torch.linalg.inv(var_matrix) @ M_tensor.T @ S0 @ init_state\n",
    "        mu_actor = actor.get_action(init_state)\n",
    "        error = torch.norm(mu_actor - mu_star).item()\n",
    "        error_list.append(error)\n",
    "\n",
    "        # 打印日志\n",
    "        if (ep + 1) % 50 == 0:\n",
    "            print(f\"Episode {ep + 1}/{num_episodes}, Loss: {loss.item():.4f}, Actor Mean Error: {error:.4f}\")\n",
    "\n",
    "    # 绘制误差曲线\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(error_list)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Actor Mean Error (||μ_actor - μ*||)\")\n",
    "    plt.title(\"Actor Learning: Mean Error vs Episodes\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return actor\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. 运行 Actor-only 算法\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    trained_actor = train_actor(num_episodes=500)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log版第四题\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy.integrate import solve_ivp, cumulative_trapezoid as cumtrapz\n",
    "# 用 cumtrapz 时\n",
    "from scipy.integrate import solve_ivp, cumulative_trapezoid\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. Soft_LQR 类：求解 S(t) 并提供最优值函数\n",
    "# ==========================================\n",
    "class Soft_LQR:\n",
    "    def __init__(self, H, M, C, D, R, sigma, T, N, tau, gamma):\n",
    "        \"\"\"\n",
    "        参考文献 [1] 中对 LQR 的松弛熵正则形式，使用 Riccati ODE 求解 S(t)，\n",
    "        并预先计算积分 I(t) = ∫ tr(σσ^T S(r)) dr.\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.R = R\n",
    "        self.sigma = sigma\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.time_grid = torch.linspace(0, T, N + 1)\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.S_values = self.solve_riccati_ode()\n",
    "        self.integral_values = self.precompute_integrals()\n",
    "\n",
    "    def riccati_ode(self, t, S_flat):\n",
    "        \"\"\"\n",
    "        这是文献 [1] 对连续时间 Riccati 方程的实现。\n",
    "        S_dot = (S^T M (D+...)^{-1} M^T S) - H^T S - S H - C\n",
    "        \"\"\"\n",
    "        S = torch.tensor(S_flat, dtype=torch.float32).reshape(2, 2)\n",
    "        D_term = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        S_dot = (S.T @ self.M @ torch.linalg.inv(D_term) @ self.M.T @ S\n",
    "                 - self.H.T @ S - S @ self.H - self.C)\n",
    "        return S_dot.flatten()\n",
    "\n",
    "    def solve_riccati_ode(self):\n",
    "        \"\"\"\n",
    "        从 T 往 0 反向求解 Riccati ODE，然后再翻转回来。\n",
    "        \"\"\"\n",
    "        S_T = self.R.flatten()\n",
    "        indices = torch.arange(self.time_grid.size(0) - 1, -1, -1)\n",
    "        time_grid_re = torch.index_select(self.time_grid, 0, indices)\n",
    "        sol = solve_ivp(\n",
    "            self.riccati_ode, [self.T, 0], S_T, t_eval=time_grid_re,\n",
    "            atol=1e-10, rtol=1e-10\n",
    "        )\n",
    "        S_matrices = sol.y.T[::-1].reshape(-1, 2, 2)\n",
    "        return dict(zip(tuple(self.time_grid.tolist()), S_matrices))\n",
    "\n",
    "    def precompute_integrals(self):\n",
    "        \"\"\"\n",
    "        I(t_i) = ∫_{t_i}^{T} tr(σσ^T S(r)) dr\n",
    "        使用累计梯形法数值近似。\n",
    "        \"\"\"\n",
    "        times = self.time_grid.numpy()\n",
    "        f_vals = []\n",
    "        for t in times:\n",
    "            S_t = self.S_values[t]\n",
    "            f_val = torch.trace(self.sigma @ self.sigma.T @ torch.tensor(S_t, dtype=torch.float32)).item()\n",
    "            f_vals.append(f_val)\n",
    "        f_vals = np.array(f_vals)\n",
    "        dt = times[1] - times[0]\n",
    "        cum_int = cumulative_trapezoid(f_vals[::-1], dx=dt, initial=0)[::-1]\n",
    "        integral_dict = {t: I for t, I in zip(times, cum_int)}\n",
    "        return integral_dict\n",
    "\n",
    "    def get_nearest_S(self, t):\n",
    "        \"\"\"\n",
    "        给定一个 t，在离散网格 self.time_grid 上找最近的节点，并返回对应的 S(t)。\n",
    "        \"\"\"\n",
    "        nearest_t = self.time_grid[torch.argmin(torch.abs(self.time_grid - t))]\n",
    "        return self.S_values[nearest_t.tolist()]\n",
    "\n",
    "    def value_function(self, t, x):\n",
    "        \"\"\"\n",
    "        v*(t,x) = x^T S(t) x + I(t) + (T - t)*C_{D,tau,gamma}\n",
    "        \"\"\"\n",
    "        S_t = torch.tensor(self.get_nearest_S(t), dtype=torch.float32)\n",
    "        val = x.T @ S_t @ x\n",
    "\n",
    "        times = self.time_grid.numpy()\n",
    "        idx = np.argmin(np.abs(times - t))\n",
    "        t_nearest = times[idx]\n",
    "        val += self.integral_values[t_nearest]\n",
    "\n",
    "        var_matrix = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        inv_matrix = torch.linalg.inv(var_matrix)\n",
    "        det_matrix = torch.det(inv_matrix)\n",
    "        C_const = - self.tau * torch.log((self.tau / self.gamma ** 2) * torch.sqrt(det_matrix))\n",
    "        val += (self.T - t) * C_const\n",
    "        return val\n",
    "\n",
    "    def optimal_control(self, t, x):\n",
    "        \"\"\"\n",
    "        返回理论最优分布：\n",
    "          N( -inv(D+tau/(2 gamma^2)) M^T S(t) x,  tau*(D+tau/(2 gamma^2)) ).\n",
    "        \"\"\"\n",
    "        S_t = torch.tensor(self.get_nearest_S(t), dtype=torch.float32)\n",
    "        inv_term = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        mean_control = -torch.linalg.inv(inv_term) @ self.M.T @ S_t @ x\n",
    "        cov_control = self.tau * inv_term\n",
    "        return MultivariateNormal(mean_control, cov_control)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Actor 策略：θ 是一个 2x2，可学习参数\n",
    "# ==========================================\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_dim=2, action_dim=2):\n",
    "        \"\"\"\n",
    "        简单地将策略的均值参数化为 mu = θ x, 其中 θ 是 (2,2) 矩阵。\n",
    "        协方差固定为 cov_opt。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.theta = torch.nn.Parameter(torch.randn(action_dim, state_dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 形状 (2,) -> mu 形状 (2,)\n",
    "        mu = self.theta @ x\n",
    "        return mu\n",
    "\n",
    "    def get_distribution(self, x, cov):\n",
    "        mu = self.forward(x)\n",
    "        return MultivariateNormal(mu, cov)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 采样一条轨迹 (Actor-only)\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def simulate_actor_episode(actor, H, M, sigma, dt, T, x0, cov):\n",
    "    \"\"\"\n",
    "    - 不对环境状态演化做梯度，只对 log_prob(a) 可微\n",
    "    - 但这里为了安全，使用 @torch.no_grad() 包裹整个函数；\n",
    "      如果你想对 log_prob(a) 求梯度，需要先在函数里 .rsample() 在 no_grad() 外面。\n",
    "    - 返回:\n",
    "        traj: 每个离散时刻的状态\n",
    "        log_probs: 动作的 log 概率 (可微/或先保存后 detach)\n",
    "        costs: 每步 cost\n",
    "    \"\"\"\n",
    "    traj = [x0.clone()]\n",
    "    log_probs = []\n",
    "    costs = []\n",
    "\n",
    "    x = x0.clone()\n",
    "    num_steps = int(T / dt)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # 1) 先脱离 no_grad: 让动作的 log_prob 对 θ 可微\n",
    "        with torch.enable_grad():\n",
    "            x_ = x.detach().clone().requires_grad_(False)\n",
    "            dist = actor.get_distribution(x_, cov)\n",
    "            a = dist.rsample()            # a 对 θ 可微\n",
    "            logp = dist.log_prob(a)       # logp 对 θ 可微\n",
    "\n",
    "        # 把 logp 存下来(这部分梯度后续要用)\n",
    "        log_probs.append(logp)\n",
    "\n",
    "        # 2) 环境更新 + cost 计算 (no_grad)\n",
    "        cost = (x @ C_tensor @ x) + (a @ D_tensor @ a)\n",
    "        costs.append(cost.item())\n",
    "        noise = torch.randn(2)\n",
    "        x = x + (H @ x + M @ a) * dt + sigma @ (noise * torch.sqrt(torch.tensor(dt)))\n",
    "\n",
    "        traj.append(x.clone())\n",
    "\n",
    "    return traj, log_probs, costs\n",
    "\n",
    "# ==========================================\n",
    "# 4. Actor-only 算法\n",
    "# ==========================================\n",
    "def train_actor(num_episodes=500, dt=0.01, T=1.0, gamma_lr=1e-4):\n",
    "    \"\"\"\n",
    "    在纯 Actor-only 场景下，根据文献 [1] 的公式:\n",
    "      dtheta ~ - sum_n [ log_prob(a_n)*(Delta v(n) + cost(n)*dt + tau*log_prob(a_n)*dt ) ]\n",
    "    这里 v*(·) 用 Soft_LQR 里的解析解当 baseline (Exercise 2).\n",
    "    \"\"\"\n",
    "    # 环境参数\n",
    "    H_np = np.array([[0.5, 0.5],\n",
    "                     [0.0, 0.5]], dtype=np.float32)\n",
    "    M_np = np.array([[1.0, 1.0],\n",
    "                     [0.0, 1.0]], dtype=np.float32)\n",
    "    sigma_np = np.eye(2, dtype=np.float32) * 0.5\n",
    "    C_np = np.array([[1.0, 0.1],\n",
    "                     [0.1, 1.0]], dtype=np.float32)\n",
    "    D_np = np.array([[1.0, 0.1],\n",
    "                     [0.1, 1.0]], dtype=np.float32) * 0.1\n",
    "    R_terminal_np = np.array([[1.0, 0.3],\n",
    "                              [0.3, 1.0]], dtype=np.float32) * 10.0\n",
    "    tau = 0.1\n",
    "    gamma_param = 10.0\n",
    "\n",
    "    # 时间离散\n",
    "    N_steps = int(T / dt)\n",
    "    global C_tensor, D_tensor\n",
    "    C_tensor = torch.tensor(C_np)\n",
    "    D_tensor = torch.tensor(D_np)\n",
    "\n",
    "    H_tensor = torch.tensor(H_np)\n",
    "    M_tensor = torch.tensor(M_np)\n",
    "    sigma_tensor = torch.tensor(sigma_np)\n",
    "\n",
    "    # 初始化 Soft_LQR\n",
    "    soft_lqr = Soft_LQR(H_tensor, M_tensor, C_tensor, D_tensor,\n",
    "                        torch.tensor(R_terminal_np),\n",
    "                        sigma_tensor, T, N_steps, tau, gamma_param)\n",
    "\n",
    "    # 策略的协方差固定\n",
    "    var_matrix = D_tensor + tau / (2 * (gamma_param ** 2)) * torch.eye(2)\n",
    "    cov_opt = tau * var_matrix\n",
    "\n",
    "    # 初始化 Actor\n",
    "    actor = Actor()\n",
    "    optimizer = torch.optim.AdamW(actor.parameters(), lr=gamma_lr, weight_decay=1e-4)\n",
    "\n",
    "    # 用于记录每个 episode 的误差 + “log(cost)”\n",
    "    error_list = []\n",
    "    log_cost_list = []\n",
    "\n",
    "    # 初始状态采样\n",
    "    def get_init_state():\n",
    "        return torch.tensor([np.random.uniform(-2, 2),\n",
    "                             np.random.uniform(-2, 2)], dtype=torch.float32)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        init_state = get_init_state()\n",
    "        # 采样轨迹 (注意 simulate_actor_episode 里对动作 log_prob 可微)\n",
    "        traj, log_probs, costs = simulate_actor_episode(actor, H_tensor, M_tensor, sigma_tensor, dt, T,\n",
    "                                                        init_state, cov_opt)\n",
    "\n",
    "        # 计算 v*(t_n, x_n) 和 delta_v\n",
    "        v_star = []\n",
    "        for n, x_n in enumerate(traj):\n",
    "            t_n = n * dt\n",
    "            v_star.append(soft_lqr.value_function(t_n, x_n))\n",
    "        delta_v = []\n",
    "        for n in range(len(traj) - 1):\n",
    "            delta_v.append(v_star[n+1] - v_star[n])\n",
    "\n",
    "        # 计算 policy gradient 的 loss\n",
    "        #  loss = - sum_n [ logp_n * (Delta_v(n) + cost(n)*dt + tau * logp_n * dt ) ]\n",
    "        #  这里用 Python 的累加，然后最后做一次 .sum()。\n",
    "        #  也可以直接累加到同一个 tensor 里。\n",
    "        losses = []\n",
    "        for n in range(len(delta_v)):\n",
    "            term = (log_probs[n] * delta_v[n]\n",
    "                    + log_probs[n] * costs[n] * dt\n",
    "                    + tau * (log_probs[n]**2) * dt)\n",
    "            # 注意：要对这个 term 取负号，做梯度下降\n",
    "            losses.append(-term)\n",
    "\n",
    "        # 把所有 step 的 loss 累加\n",
    "        total_loss = torch.stack(losses).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算与理论最优均值的误差\n",
    "        S0 = torch.tensor(soft_lqr.get_nearest_S(0), dtype=torch.float32)\n",
    "        mu_star = -torch.linalg.inv(var_matrix) @ M_tensor.T @ S0 @ init_state\n",
    "        mu_actor = actor(init_state)\n",
    "        error = torch.norm(mu_actor - mu_star).item()\n",
    "        error_list.append(error)\n",
    "\n",
    "        # 记录一下本 episode 的总 cost，并取对数\n",
    "        # sum_of_costs = \\sum_n cost[n]*dt (近似轨迹总成本)\n",
    "        ep_cost = sum(costs)*dt\n",
    "        if ep_cost <= 0:\n",
    "            ep_cost = 1e-8  # 防止 log(0)\n",
    "        log_cost_list.append(np.log(ep_cost))\n",
    "\n",
    "        # 打印日志\n",
    "        if (ep+1) % 50 == 0:\n",
    "            print(f\"Episode {ep+1}/{num_episodes}, \"\n",
    "                  f\"TotalLoss={total_loss.item():.4f}, \"\n",
    "                  f\"MeanError={error:.4f}, \"\n",
    "                  f\"logCost={log_cost_list[-1]:.4f}\")\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(10,4))\n",
    "\n",
    "    # 1) Actor Mean Error\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(error_list, label=\"Actor Mean Error\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Error ||μ_actor - μ*||\")\n",
    "    plt.title(\"Actor Mean Error vs Episodes\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2) Log of Cost\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(log_cost_list, label=\"log cost\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"log cost\")\n",
    "    plt.title(\"Actor log of cost over episodes\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return actor\n",
    "\n",
    "# ==========================================\n",
    "# 5. 运行 Actor-only\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    trained_actor = train_actor(\n",
    "        num_episodes=500,\n",
    "        dt=0.01,\n",
    "        T=1.0,\n",
    "        gamma_lr=1e-4  # 可根据需要调节\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
