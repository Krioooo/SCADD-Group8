{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#报错的第五题\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp, cumulative_trapezoid as cumtrapz\n",
    "\n",
    "# ==========================================\n",
    "# 1. Soft_LQR 类（Exercise 2 实现的代码）——改进版\n",
    "# ==========================================\n",
    "class Soft_LQR:\n",
    "    def __init__(self, H, M, C, D, R, sigma, T, N, tau, gamma):\n",
    "        \"\"\"\n",
    "        初始化 soft LQR 类：\n",
    "          - 计算 S(t)（通过 Riccati ODE 求解）\n",
    "          - 预计算积分 I(t) = ∫[t,T] tr(σσ^T S(r)) dr\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.R = R\n",
    "        self.sigma = sigma\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.time_grid = torch.linspace(0, T, N + 1)\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.S_values = self.solve_riccati_ode()\n",
    "        self.integral_values = self.precompute_integrals()\n",
    "\n",
    "    def riccati_ode(self, t, S_flat):\n",
    "        S = torch.tensor(S_flat, dtype=torch.float32).reshape(2, 2)\n",
    "        D_term = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        S_dot = S.T @ self.M @ torch.linalg.inv(D_term) @ self.M.T @ S - self.H.T @ S - S @ self.H - self.C\n",
    "        return S_dot.flatten()\n",
    "\n",
    "    def solve_riccati_ode(self):\n",
    "        S_T = self.R.flatten()\n",
    "        indices = torch.arange(self.time_grid.size(0) - 1, -1, -1)\n",
    "        time_grid_re = torch.index_select(self.time_grid, 0, indices)\n",
    "        sol = solve_ivp(self.riccati_ode, [self.T, 0], S_T, t_eval=time_grid_re, atol=1e-10, rtol=1e-10)\n",
    "        S_matrices = sol.y.T[::-1].reshape(-1, 2, 2)\n",
    "        return dict(zip(tuple(self.time_grid.tolist()), S_matrices))\n",
    "\n",
    "    def precompute_integrals(self):\n",
    "        times = self.time_grid.numpy()\n",
    "        f_vals = []\n",
    "        for t in times:\n",
    "            S_t = self.S_values[t]\n",
    "            f_val = torch.trace(self.sigma @ self.sigma.T @ torch.tensor(S_t, dtype=torch.float32)).item()\n",
    "            f_vals.append(f_val)\n",
    "        f_vals = np.array(f_vals)\n",
    "        dt = times[1] - times[0]\n",
    "        cum_int = cumtrapz(f_vals[::-1], dx=dt, initial=0)[::-1]\n",
    "        integral_dict = {t: I for t, I in zip(times, cum_int)}\n",
    "        return integral_dict\n",
    "\n",
    "    def get_nearest_S(self, t):\n",
    "        nearest_t = self.time_grid[torch.argmin(torch.abs(self.time_grid - t))]\n",
    "        return self.S_values[nearest_t.tolist()]\n",
    "\n",
    "    def value_function(self, t, x):\n",
    "        S_t = self.get_nearest_S(t)\n",
    "        S_t = torch.tensor(S_t, dtype=torch.float32)\n",
    "        val = x.T @ S_t @ x\n",
    "        times = self.time_grid.numpy()\n",
    "        idx = np.argmin(np.abs(times - t))\n",
    "        t_nearest = times[idx]\n",
    "        val = val + self.integral_values[t_nearest]\n",
    "        var_matrix = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        inv_matrix = torch.linalg.inv(var_matrix)\n",
    "        det_matrix = torch.det(inv_matrix)\n",
    "        C_const = - self.tau * torch.log((self.tau / self.gamma ** 2) * torch.sqrt(det_matrix))\n",
    "        val = val + (self.T - t) * C_const\n",
    "        return val\n",
    "\n",
    "    def optimal_control(self, t, x):\n",
    "        S_t = self.get_nearest_S(t)\n",
    "        S_t = torch.tensor(S_t, dtype=torch.float32)\n",
    "        inv_term = self.D + self.tau / (2 * (self.gamma ** 2)) * torch.eye(2)\n",
    "        mean_control = -torch.linalg.inv(inv_term) @ self.M.T @ S_t @ x\n",
    "        cov_control = self.tau * inv_term\n",
    "        control_dist = MultivariateNormal(mean_control, cov_control)\n",
    "        return control_dist\n",
    "\n",
    "# ==========================================\n",
    "# 2. Actor 策略：参数化为线性映射 θ (2x2 矩阵)\n",
    "# ==========================================\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim=2, action_dim=2):\n",
    "        super(Actor, self).__init__()\n",
    "        # 参数 θ 初始化为随机 2x2 矩阵\n",
    "        self.theta = nn.Parameter(torch.randn(action_dim, state_dim, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 输入 x 为形状 (2,) 向量\n",
    "        mu = (self.theta @ x).squeeze()  # 输出形状 (2,)\n",
    "        return mu\n",
    "\n",
    "    def get_distribution(self, x, cov):\n",
    "        mu = self.forward(x)\n",
    "        return MultivariateNormal(mu, cov)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Critic 模型：近似价值函数 v(t,x;η)\n",
    "# ==========================================\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256):\n",
    "        # 输入为 [t, x0, x1]\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, t, x):\n",
    "        # t 为标量，x 为形状 (2,) 向量\n",
    "        # 拼接成 (3,) 向量\n",
    "        t_tensor = torch.tensor([t], dtype=torch.float32, device=x.device)\n",
    "        inp = torch.cat([t_tensor, x], dim=0)\n",
    "        return self.net(inp).squeeze()  # 输出标量\n",
    "\n",
    "# ==========================================\n",
    "# 4. 利用当前 actor 策略生成一条轨迹（Euler 离散化）\n",
    "# ==========================================\n",
    "def simulate_actor_episode(actor, H, M, sigma, dt, T, x0, cov):\n",
    "    # 我们假设 x0 为形状 (2,) 向量\n",
    "    traj = [x0.detach().clone()]  # 记录状态\n",
    "    log_probs = []\n",
    "    costs = []\n",
    "    # 记录每步的时间\n",
    "    times = [0.0]\n",
    "    \n",
    "    x = x0\n",
    "    num_steps = int(T / dt)\n",
    "    for n in range(num_steps):\n",
    "        t = n * dt\n",
    "        # 生成动作，a 形状为 (2,)\n",
    "        dist = actor.get_distribution(x, cov)\n",
    "        a = dist.rsample()\n",
    "        logp = dist.log_prob(a)\n",
    "        log_probs.append(logp)\n",
    "        \n",
    "        dt_tensor = torch.tensor(dt, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            # 计算即时 cost: cost = x^T C x + a^T D a\n",
    "            cost = (x @ C_tensor @ x) + (a @ D_tensor @ a)\n",
    "            costs.append(cost.item())\n",
    "            noise = torch.randn(2)\n",
    "            x = x + (H @ x + M @ a) * dt + sigma @ (noise * torch.sqrt(dt_tensor))\n",
    "        traj.append(x.detach().clone())\n",
    "        times.append((n+1)*dt)\n",
    "    return traj, log_probs, costs, times\n",
    "\n",
    "# ==========================================\n",
    "# 5. Actor–Critic 算法主训练函数\n",
    "# ==========================================\n",
    "def train_actor_critic(num_episodes=500, dt=0.01, T=1.0, actor_lr=1e-4, critic_lr=1e-1):\n",
    "    # 固定环境参数（与 main() 中一致）\n",
    "    H_np = np.array([[0.5, 0.5],\n",
    "                     [0.0, 0.5]], dtype=np.float32)\n",
    "    M_np = np.array([[1.0, 1.0],\n",
    "                     [0.0, 1.0]], dtype=np.float32)\n",
    "    sigma_np = np.eye(2, dtype=np.float32) * 0.5\n",
    "    C_np = np.array([[1.0, 0.1],\n",
    "                     [0.1, 1.0]], dtype=np.float32)\n",
    "    D_np = np.array([[1.0, 0.1],\n",
    "                     [0.1, 1.0]], dtype=np.float32) * 0.1\n",
    "    R_terminal_np = np.array([[1.0, 0.3],\n",
    "                              [0.3, 1.0]], dtype=np.float32) * 10.0\n",
    "    tau = 0.1\n",
    "    gamma_param = 10.0\n",
    "    N_steps = int(T / dt)\n",
    "    \n",
    "    # 转换为 torch 张量\n",
    "    H_tensor = torch.tensor(H_np)\n",
    "    M_tensor = torch.tensor(M_np)\n",
    "    sigma_tensor = torch.tensor(sigma_np)\n",
    "    global C_tensor, D_tensor, R_tensor\n",
    "    C_tensor = torch.tensor(C_np)\n",
    "    D_tensor = torch.tensor(D_np)\n",
    "    R_tensor = torch.tensor(R_terminal_np)\n",
    "    \n",
    "    # 初始化 Soft_LQR\n",
    "    soft_lqr = Soft_LQR(H_tensor, M_tensor, C_tensor, D_tensor, R_tensor, sigma_tensor, T, N_steps,\n",
    "                        tau, gamma_param)\n",
    "    \n",
    "    # 固定策略协方差\n",
    "    var_matrix = D_tensor + tau / (2 * (gamma_param ** 2)) * torch.eye(2)\n",
    "    cov_opt = tau * var_matrix\n",
    "    \n",
    "    # 初始化 Actor、Critic 及其优化器\n",
    "    actor = Actor(state_dim=2, action_dim=2)\n",
    "    critic = Critic(input_dim=3, hidden_dim=256)\n",
    "    optimizer_actor = torch.optim.AdamW(actor.parameters(), lr=actor_lr, weight_decay=1e-4)\n",
    "    optimizer_critic = torch.optim.AdamW(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    # 初始状态生成函数：返回形状 (2,) 的向量\n",
    "    def get_init_state():\n",
    "        return torch.tensor([np.random.uniform(-2, 2), np.random.uniform(-2, 2)], dtype=torch.float32)\n",
    "    \n",
    "    error_list = []\n",
    "    actor_loss_list = []\n",
    "    critic_loss_list = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        init_state = get_init_state()\n",
    "        traj, log_probs, costs, times = simulate_actor_episode(actor, H_tensor, M_tensor, sigma_tensor, dt, T, init_state, cov_opt)\n",
    "        num_steps_sim = len(traj)\n",
    "        \n",
    "        # --- 计算 Critic 预测 ---\n",
    "        v_hat = []\n",
    "        for n in range(num_steps_sim):\n",
    "            t_n = times[n]\n",
    "            x_n = traj[n]\n",
    "            v_hat.append(critic(t_n, x_n))\n",
    "        # v_hat 为长度 num_steps_sim 的列表，每个元素为标量 tensor\n",
    "        \n",
    "        # --- 计算 Monte-Carlo 回报目标 G_n ---\n",
    "        # 设终端成本为 g(x_T) = x_T^T R x_T\n",
    "        terminal_cost = traj[-1] @ R_tensor @ traj[-1]\n",
    "        G_target = []  # 长度为 num_steps_sim\n",
    "        # 从末尾反向累加回报\n",
    "        G = terminal_cost.item()\n",
    "        G_target.insert(0, G)\n",
    "        for k in range(num_steps_sim-2, -1, -1):\n",
    "            # 使用 cost[k] 和对应的 log_prob（注意：critic训练时把 log_prob 作为常数处理）\n",
    "            G = (costs[k] + tau * log_probs[k].detach().item()) * dt + G\n",
    "            G_target.insert(0, G)\n",
    "        # 将 G_target 转成 tensor\n",
    "        G_target = torch.tensor(G_target, dtype=torch.float32)\n",
    "        \n",
    "        # --- Critic Loss: 均方误差 ---\n",
    "        critic_loss = 0\n",
    "        for n in range(num_steps_sim):\n",
    "            critic_loss = critic_loss + (v_hat[n] - G_target[n])**2\n",
    "        critic_loss = critic_loss / num_steps_sim\n",
    "        \n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        # --- Actor Loss ---\n",
    "        # 根据算法 3：使用 TD 差分近似\n",
    "        actor_loss = 0\n",
    "        for n in range(num_steps_sim - 1):\n",
    "            delta_v = v_hat[n+1] - v_hat[n]\n",
    "            # 注意：log_prob 本身是可微的\n",
    "            actor_loss_term = log_probs[n] * (delta_v + (costs[n] + tau * log_probs[n]) * dt)\n",
    "            actor_loss = actor_loss - actor_loss_term  # 因为我们希望上升梯度方向\n",
    "        actor_loss = actor_loss / (num_steps_sim - 1)\n",
    "        \n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        optimizer_actor.step()\n",
    "        \n",
    "        # 记录误差：这里用当前 actor 与理论最优均值（来自 Soft_LQR 的最优控制）比较\n",
    "        S0 = torch.tensor(soft_lqr.get_nearest_S(0), dtype=torch.float32)\n",
    "        mu_star = -torch.linalg.inv(var_matrix) @ M_tensor.T @ S0 @ init_state.unsqueeze(1)\n",
    "        mu_actor = actor(init_state).unsqueeze(1)\n",
    "        error = torch.norm(mu_actor - mu_star).item()\n",
    "        error_list.append(error)\n",
    "        actor_loss_list.append(actor_loss.item())\n",
    "        critic_loss_list.append(critic_loss.item())\n",
    "        \n",
    "        if (ep + 1) % 50 == 0:\n",
    "            print(f\"Episode {ep+1}/{num_episodes}, Actor Loss: {actor_loss.item():.6f}, Critic Loss: {critic_loss.item():.6f}, Error: {error:.6f}\")\n",
    "    \n",
    "    # 绘制误差及损失曲线\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(error_list)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Actor Mean Error\")\n",
    "    plt.title(\"Actor Mean Error vs Episodes\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(actor_loss_list, label=\"Actor Loss\")\n",
    "    plt.plot(critic_loss_list, label=\"Critic Loss\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Episodes\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return actor, critic\n",
    "\n",
    "# ==========================================\n",
    "# 6. 运行 Actor–Critic 算法\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    trained_actor, trained_critic = train_actor_critic(num_episodes=500, dt=0.01, T=1.0, actor_lr=1e-4, critic_lr=1e-1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
